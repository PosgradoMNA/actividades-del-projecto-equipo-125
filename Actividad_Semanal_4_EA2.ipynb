{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/actividades-del-projecto-equipo-125/blob/main/Actividad_Semanal_4_EA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actividad Semanal - 4\n",
        "\n",
        "Estefania Abigail Castro Belmont A01332636\n",
        "\n",
        "Vladimir Salazar Altamirano A01793118\n",
        "\n",
        "Ciencia y analítica de datos\n",
        "\n",
        "Jobish\n",
        "\n",
        "11/10/2022"
      ],
      "metadata": {
        "id": "dW9tJhNxCA5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Descripción del entendimiento de los 3 puntos indicados en la parte 1"
      ],
      "metadata": {
        "id": "KngzWfMCCPCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1: Determine el número mínimo de componentes principales que representan la mayor parte de la variación en sus datos\n",
        "\n",
        "Utilice la proporción acumulada de la varianza que explican los componentes para determinar la cantidad de varianza que explican los componentes principales."
      ],
      "metadata": {
        "id": "uyuMSbvKC4ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2: Interprete cada componente principal en términos de las variables originales\n",
        "\n",
        "Examine la magnitud y la dirección de los coeficientes de las variables originales.\n",
        "Nota: Cuanto mayor sea el valor absoluto del coeficiente, más importante será la variable correspondiente en el cálculo del componente."
      ],
      "metadata": {
        "id": "2eadqdALC_-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Identifique valores atípicos\n",
        "\n",
        "Realice alguna gráfica de valores atípicos o boxplot para identificar los valores atípicos. Cualquier punto que esté más alejado de la línea de referencia es un valor atípico."
      ],
      "metadata": {
        "id": "PcSRbt2hDB4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parte 1: Ejercicio guiado"
      ],
      "metadata": {
        "id": "p31gUIDoDT84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como la última ocasión ya habíamos realizado preprocesamiento de la base de datos tomamos el archivo limpio, donde hicimos ya algunas eliminaciones y completamos los datos faltantes para no repetir el código que ya se había empleado en el ejercicio anterior\n"
      ],
      "metadata": {
        "id": "s9E4zRDIELQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL2msfskLYtT"
      },
      "outputs": [],
      "source": [
        "#importamos librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leemos el archivo csv\n",
        "df = pd.read_csv(\"https://github.com/PosgradoMNA/actividades-del-projecto-equipo-125/blob/main/datoslimpios_default_of_credit_card_clients.csv\")"
      ],
      "metadata": {
        "id": "hFQRNlWVLitZ",
        "outputId": "8177100f-15c2-403e-ef9e-c77be2a51693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b64ddb5314d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Leemos el archivo csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/PosgradoMNA/actividades-del-projecto-equipo-125/blob/main/datoslimpios_default_of_credit_card_clients.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 28, saw 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pasamos a DataFrame\n",
        "df = pd.DataFrame(df)"
      ],
      "metadata": {
        "id": "7yXybtjVLolg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aquí desplegamos el dataframe con las primeras cinco filas para visualizar datos.\n",
        "#Se observa que la última columna es la \"Y\".\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "XBU534A64haw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vemos las filas y columnas del df\n",
        "df.shape"
      ],
      "metadata": {
        "id": "stwcvjJeMQ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quitamos los datos que nos estorbarían en el ejercicio"
      ],
      "metadata": {
        "id": "YCvOt_baw42g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copiamos los datos originales para no transofrmarlos\n",
        "ndf= df.copy()"
      ],
      "metadata": {
        "id": "TwrCVjtdw-mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vemos las columnas que conforman el df\n",
        "ndf.columns"
      ],
      "metadata": {
        "id": "l9h2KjL6xA4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Eliminamos las dos columnas que tienen doble índice y la columna target\n",
        "ndf = ndf.drop(columns = ['Unnamed: 0', 'ID', 'Y'])"
      ],
      "metadata": {
        "id": "_X6RT1LUxDhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vemos las filas y columnas del df final\n",
        "ndf.shape"
      ],
      "metadata": {
        "id": "sp_HVWorFMLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Paso 1 \n",
        "Determine el número mínimo de componentes principales que representan la mayor parte de la variación en sus datos\n",
        "\n",
        "Utilice la proporción acumulada de la varianza que explican los componentes para determinar la cantidad de varianza que explican los componentes principales."
      ],
      "metadata": {
        "id": "NE77Yp3HMI1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vemos la matriz de correlación para ver el comportamiento de las variables\n",
        "import seaborn as singular_values_\n",
        "correlacion = ndf.corr().round(2)\n",
        "\n",
        "sns.set(rc= {\"figure.figsize\":(15,10)})\n",
        "sns.heatmap(correlacion, vmin = -1, vmax = 1, cmap = \"BuGn\", annot = True)"
      ],
      "metadata": {
        "id": "cq53jq83L9Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reescalamos el ndf para que los datos queden normalizados\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(ndf)\n",
        "#Son muchos valores. Así que imprimamos los primeros 5 resultados mejor.\n",
        "scaled[:5]\n",
        "scaled_df = pd.DataFrame(scaled, columns=ndf.columns)\n",
        "scaled_df.head()\n"
      ],
      "metadata": {
        "id": "B4TVaXvo_lAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HAcemos el fit a PCA\n",
        "from sklearn import preprocessing\n",
        "pcs = PCA()\n",
        "pcs.fit(preprocessing.scale(ndf))"
      ],
      "metadata": {
        "id": "AGXJ01PgwfkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "summary = {'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
        "                          'Proportion of variance': pcs.explained_variance_ratio_,\n",
        "                          'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)#column \n",
        "                          }\n",
        "pcsSummary = pd.DataFrame(summary).transpose()\n",
        "pcsSummary = pcsSummary.round(2)\n",
        "pcsSummary"
      ],
      "metadata": {
        "id": "20U71lmuwinB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PC_components = np.arange(pcs.n_components_) + 1\n",
        "cusm = np.cumsum(pcs.explained_variance_ratio_)\n",
        "vartio = pcs.explained_variance_ratio_\n",
        "\n",
        "_ = sns.set(style = 'whitegrid', \n",
        "            font_scale = 1.2\n",
        "            )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "_ = sns.barplot(x = PC_components, \n",
        "                y = pcs.explained_variance_ratio_, \n",
        "                color = 'b'\n",
        "                )\n",
        "\n",
        "_ = sns.lineplot(x = PC_components-1, \n",
        "                 y = np.cumsum(pcs.explained_variance_ratio_), \n",
        "                 color = 'black', \n",
        "                 linestyle = '-', \n",
        "                 linewidth = 2, \n",
        "                 marker = 'o', \n",
        "                 markersize = 8\n",
        "                 )\n",
        "\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('N-th Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7KPgczFawqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcsSummary.columns = [\"PC{}\".format(i) for i in range(1,len(pcsSummary.columns)+1)]\n",
        "pcsSummary.columns\n",
        "\n",
        "pcsCompsDF = pd.DataFrame(pcs.components_.transpose(),\n",
        "                          columns = pcsSummary.columns,\n",
        "                          index = ndf.columns )\n",
        "pcsCompsDF.iloc[:,:14]"
      ],
      "metadata": {
        "id": "HTA9aELuwwIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcsSummary.iloc[2,14]"
      ],
      "metadata": {
        "id": "RUbCclS9wzqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcs_labels = [f'PC{i + 1}' for i in range(len(scaled_df.columns))]\n",
        "pcsSummary_df.index = pcs_labels\n",
        "pcsSummary_df"
      ],
      "metadata": {
        "id": "VUhq5M51_KZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Respuesta\n",
        "Decidimos usar 14 componentes, ya que con ello alcanzamos una variabilidad del 96%. Tal como podemos ver en la gráfica del Scree Plot y en el DF pcsSummary, en el tercer renglon, proporcion acumulada. "
      ],
      "metadata": {
        "id": "bMhBKt_s81tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Paso 2\n",
        "Interprete cada componente principal en términos de las variables originales\n",
        "\n",
        "Examine la magnitud y la dirección de los coeficientes de las variables originales.\n",
        "Nota: Cuanto mayor sea el valor absoluto del coeficiente, más importante será la variable correspondiente en el cálculo del componente."
      ],
      "metadata": {
        "id": "ndN8hruH8lVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pcsSummary.columns = [\"PC{}\".format(i) for i in range(1,len(pcsSummary.columns)+1)]\n",
        "pcsSummary.columns\n",
        "\n",
        "pcsCompsDF = pd.DataFrame(pcs.components_.transpose(),\n",
        "                          columns = pcsSummary.columns,\n",
        "                          index = ndf.columns )\n",
        "pcsCompsDF.iloc[:,:14].round(2)"
      ],
      "metadata": {
        "id": "_4ZuepLG9ZeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Buscamos la varianza total de todas las variables\n",
        "total_var = df.var().sum()"
      ],
      "metadata": {
        "id": "nGfWa-di-E7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculamos la varianza para cada variable que estamos estudiando\n",
        "var_x1 = df.X1.var()\n",
        "var_x2 = df.X2.var()\n",
        "var_x3 = df.X3.var()\n",
        "var_x4 = df.X4.var()\n",
        "var_x5 = df.X5.var()\n",
        "var_x6 = df.X6.var()\n",
        "var_x7 = df.X7.var()\n",
        "var_x8 = df.X8.var()\n",
        "var_x9 = df.X9.var()\n",
        "var_x10 = df.X10.var()\n",
        "var_x11 = df.X11.var()\n",
        "var_x12 = df.X12.var()\n",
        "var_x13 = df.X13.var()\n",
        "var_x14 = df.X14.var()\n",
        "var_x15 = df.X15.var()\n",
        "var_x16 = df.X16.var()\n",
        "var_x17 = df.X17.var()\n",
        "var_x18 = df.X18.var()\n",
        "var_x19 = df.X19.var()\n",
        "var_x20 = df.X20.var()\n",
        "var_x21 = df.X21.var()\n",
        "var_x22 = df.X22.var()\n",
        "var_x23 = df.X23.var()\n",
        "var_Y = df.Y.var()"
      ],
      "metadata": {
        "id": "2lBDzIST-yml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sacamos el % que representa la varianza de cada variable respecto al total.\n",
        "print('Varianza Total \\t:',total_var.round(2) )\n",
        "print('Varianza X1 \\t:', ((var_x1/total_var)*100).round(2),'%' )\n",
        "print('Varianza X2 \\t:', ((var_x2/total_var)*100).round(2),'%' )\n",
        "print('Varianza X3 \\t:', ((var_x3/total_var)*100).round(2),'%' )\n",
        "print('Varianza X4 \\t:', ((var_x4/total_var)*100).round(2),'%' )\n",
        "print('Varianza X5 \\t:', ((var_x5/total_var)*100).round(2),'%' )\n",
        "print('Varianza X6 \\t:', ((var_x6/total_var)*100).round(2),'%' )\n",
        "print('Varianza X7 \\t:', ((var_x7/total_var)*100).round(2),'%' )\n",
        "print('Varianza X8 \\t:', ((var_x8/total_var)*100).round(2),'%' )\n",
        "print('Varianza X9 \\t:', ((var_x9/total_var)*100).round(2),'%' )\n",
        "print('Varianza X10 \\t:',((var_x10/total_var)*100).round(2),'%' )\n",
        "print('Varianza X11 \\t:', ((var_x11/total_var)*100).round(2),'%' )\n",
        "print('Varianza X12 \\t:', ((var_x12/total_var)*100).round(2),'%' )\n",
        "print('Varianza X13 \\t:', ((var_x13/total_var)*100).round(2),'%' )\n",
        "print('Varianza X14 \\t:', ((var_x14/total_var)*100).round(2),'%' )\n",
        "print('Varianza X15 \\t:', ((var_x15/total_var)*100).round(2),'%' )\n",
        "print('Varianza X16 \\t:', ((var_x16/total_var)*100).round(2),'%' )\n",
        "print('Varianza X17 \\t:', ((var_x17/total_var)*100).round(2),'%' )\n",
        "print('Varianza X18 \\t:', ((var_x18/total_var)*100).round(2),'%' )\n",
        "print('Varianza X19 \\t:', ((var_x19/total_var)*100).round(2),'%' )\n",
        "print('Varianza X20 \\t:',((var_x20/total_var)*100).round(2),'%' )\n",
        "print('Varianza X21 \\t:', ((var_x21/total_var)*100).round(2),'%' )\n",
        "print('Varianza X22 \\t:', ((var_x22/total_var)*100).round(2),'%' )\n",
        "print('Varianza X23 \\t:', ((var_x23/total_var)*100).round(2),'%' )\n",
        "print('Varianza Y \\t:', ((var_Y/total_var)*100).round(2),'%' )"
      ],
      "metadata": {
        "id": "dseZHfGi-yvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_var =scaled_df.var().sum()\n",
        "pd.DataFrame({\"Porcentaje Varianza\": (scaled_df.var()/ total_var) * 100,\n",
        "              \"Porcentaje Varianza Acumulado\": (scaled_df.var().cumsum() / total_var) *100\n",
        "})\n"
      ],
      "metadata": {
        "id": "dtx-d5u8AGt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcsCompsDF.PC1.abs().nlargest(3)"
      ],
      "metadata": {
        "id": "hOrp_sC1xgNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcsCompsDF.PC2.abs().nlargest(3)"
      ],
      "metadata": {
        "id": "UGPLFYaFxnUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcsCompsDF.iloc[:,:14].abs().idxmax()\n"
      ],
      "metadata": {
        "id": "Y3TGc0z7AbK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Respuesta"
      ],
      "metadata": {
        "id": "74pdEZKjBGX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        " \n",
        "pca = PCA(n_components=14)\n",
        " \n",
        "pca_features = pca.fit_transform(ndf)\n",
        " \n",
        "print('Shape before PCA: ', ndf.shape)\n",
        "print('Shape after PCA: ', pca_features.shape)\n",
        " \n",
        "pca_df = pd.DataFrame(\n",
        "    data=pca_features, \n",
        "    columns=['PC1', 'PC2', 'PC3','PC4', 'PC5', 'PC6','PC7', 'PC8', 'PC9','PC10', 'PC11', 'PC12', 'PC13','PC14'])"
      ],
      "metadata": {
        "id": "Ay1vDQ-j4ynl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df[\"Target\"] = df.Y\n",
        "pca_df"
      ],
      "metadata": {
        "id": "iVCcHOv649n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scale = StandardScaler()\n",
        "X = scale.fit_transform(ndf)\n",
        "\n",
        "\n",
        "kmeans =KMeans(n_clusters=2).fit(X)\n",
        "pca_df['cluster'] = pd.Categorical(kmeans.labels_)\n",
        "sns.scatterplot(x=\"PC1\",y=\"PC3\",hue=\"cluster\",data=pca_df)"
      ],
      "metadata": {
        "id": "hNwM9O6K7F2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Paso 3\n",
        "Identifique valores atípicos\n",
        "\n",
        "Realice alguna gráfica de valores atípicos o boxplot para identificar los valores atípicos. Cualquier punto que esté más alejado de la línea de referencia es un valor atípico."
      ],
      "metadata": {
        "id": "6d7bEPaz0B2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df.boxplot()"
      ],
      "metadata": {
        "id": "sDi8gLC1z2jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot()"
      ],
      "metadata": {
        "id": "1pMQInopBUOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Respuesta"
      ],
      "metadata": {
        "id": "TFVQURowDwaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlaciones= new_df.transpose().corr().round(2)\n",
        "correlaciones= correlaciones.replace(1,0)\n",
        "correlaciones"
      ],
      "metadata": {
        "id": "mv3wDVMCyd1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in correlaciones:\n",
        "    print(\"Mayor correlacion: \", np.abs(correlaciones[[c]]).idxmax(),\"% de corr________________\", np.abs(correlaciones[c]).max())"
      ],
      "metadata": {
        "id": "pL57hO0NyfHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parte 2\n",
        "Responde las siguientes preguntas en una celda de texto en Jupyter Notebook\n",
        "\n",
        "¿Cuál es el número de componentes mínimo y por qué?\n",
        "Se considera que es necesario determinar el criterio para determinar qué entendemos por \"la mayor parte de de la varianza explicada acumulada\". Ya que en función de ello se determinará el número mínimo de componente a utilizar. Por ejemplo, si consideramos que la mayor parte refiere a más de la mitad, es decir, más del 50% de la varianza explicada acumulada), entonces con tres componentes es suficiente para cumplir con dicho criterio. En cambio, si somos más estrictos e interpretamos que la mayor parte refiere al menos al 90%, entonces necesitamos 13 componentes para cumplir con dicho criterio.\n",
        "\n",
        "¿Cuál es la variación de los datos que representan esos componentes?\n",
        "\n",
        "\n",
        "¿Cuál es la pérdida de información después de realizar PCA? PCA es una técnica que nos permirte \"reducir dimensiones\" para no tener que trabajar con todas la variables. Cada una de estas componentes es una combinación de un subconjunto de todas la variables, y tiene la capacidad de explicar un determinado porcentaje de la varianza explicada acumulada.\n",
        "\n",
        "De las variables originales, ¿Cuál tiene mayor y cuál tiene menor importancia en los componentes principales?\n",
        "\n",
        "De las variables originales, \n",
        "\n",
        "¿Cuándo se recomienda realizar un PCA y qué beneficios ofrece para Machine Learning?"
      ],
      "metadata": {
        "id": "bnefQz0LBe1G"
      }
    }
  ]
}